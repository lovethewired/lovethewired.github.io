{"pageProps":{"postData":{"slug":["blog","2023","test-your-tests"],"contentRaw":"\n> This is a writeup of the talks given at [TrustX](https://www.secureum.xyz/trustx/) and [Solidity Summit](https://soliditylang.org/summit/) at [Devconnect Istanbul 2023](https://devconnect.org/). I am a blockchain security engineer and researcher at Trail of Bits.\n\n_In order to gain the most out of this article, it is advised that you actively try to discover the issue within the tests in the examples (hidden behind spoiler tags). Take a moment and consider what patterns might seem dangerous or brittle and how you could implement safer tests._\n\nIn the rapidly evolving landscape of software development, **the importance of rigorous testing cannot be overstated**. This is especially true when dealing with mission-critical systems like blockchains with millions at risk. The reliability of tests are fundamental to ensuring the integrity and security of these systems. But, _how can we guarantee robustness of our tests, and how do we evaluate their quality and efficacy?_\n\n**Table of Contents**\n\n- [The Role of Testing](#the-role-of-testing)\n- [Motivation Behind the Topic](#motivation-behind-the-topic)\n  - [Improving ERC721A](#improving-erc721a)\n  - [Development and Testing Approach](#development-and-testing-approach)\n- [Testing Shortcomings Exemplified](#testing-shortcomings-exemplified)\n  - [Testing WAD Conversions](#testing-wad-conversions)\n  - [Testing WAD Multiplication](#testing-wad-multiplication)\n  - [Exploring Various Testing Strategies](#exploring-various-testing-strategies)\n- [Offensive Testing](#offensive-testing)\n  - [Case Study: Primitive Finance - Hyper](#case-study-primitive-finance---hyper)\n  - [Initial Testing Strategy](#initial-testing-strategy)\n  - [Refining the Testing Strategy](#refining-the-testing-strategy)\n- [Conclusions and Takeaways](#conclusions-and-takeaways)\n- [Extra Material: Additional Testing Shortcomings](#extra-material-additional-testing-shortcomings)\n  - [Testing Helper Functions](#testing-helper-functions)\n  - [Tool Quirks](#tool-quirks)\n\n## The Role of Testing\n\n_Why do we even test our software?_\n\nTests are primarily written with the goal of **identifying incorrect or unexpected behavior** of our software. Unit tests are great for validating that our code will produce a certain outcome given an input. With the advancement of automated testing we are also able to **validate properties or invariants of our system** by simulating function calls with persistent state. Unit and fuzz tests can also be great for **ensuring that a piece of code adheres to its specification**. Tests for the ERC721 standard, for example, can be applied to an extension of the specification in order to uncover unexpected deviations from the requirements of the norm.\n\nWhat's more, tests act as the protective layer for preserving code functionality. A well-crafted test should **cement the anticipated code behavior**, safeguarding it against unexpected issues that might be introduced in future modifications.\n\nAlthough applying various testing methods and static analysis can vouch for your code's stability, it's important to keep in mind that **no test and testing method is infallible**. _Testing is an ongoing process of refinement, not a final endpoint!_ It is impossible to gain absolute certainty that your code will be bug-free. Tests may contain bugs themselves, fuzzers can be used ineffectively and even tools for formal verification make faulty assumptions and suffer from state explosion or bad heuristics. In order to achieve robustness in software, constant attention and multiple testing methods are essential.\n\n## Motivation Behind the Topic\n\nThis topic's inspiration stems from my own experiences and reflections on past shortcomings. Before becoming a security researcher, I paved my path as a developer. Every developer makes a mistake at some point. I wanted to learn from every single one of them. Each identified vulnerability led me to re-evaluate my approach—_how could I improve my development and testing process to prevent similar situations arising in the future?_\n\nA memorable experience in this regard remains when optimizing the [ERC721A](https://www.erc721a.org/) contract.\n\n### Improving ERC721A\n\nERC721A works by leveraging batched token mints involving sequential IDs. The idea is to defer costly storage operations at a time of high network costs (during the mint) to a time of low network costs (for subsequent transfers). This is done by keeping the ownership data of IDs minted in sequence implicit instead of explicitly committing them to the storage one by one.\n\nWhen identifying the owner of an ID, if the ownership data is empty (uncommitted/implicit), then the owner is looked up in the previous slot.\n\n```\nBob mints 5 IDs:\n               tokenId\n                  V\n| X | Bob |    |    |    |    | Y |\n       ^\n      from\n\nBob transfers `tokenId` to Eve:\n\n| X | Bob |    | Eve |    |    | Y |\n                  ^\n                  to\n\nFollowing Ownership Commit:\n\n| X | Bob |    | Eve | Bob |    | Y |\n                        ^\n```\n\nUpon transfers, ERC721A checks whether the ownership data of the subsequent ID is explicit or implicit. In order to maintain consistent ownership data, it must be set explicitly.\n\nIt's not important to understand the specifics of the implementation. All that should be known is that it aims to mimic the behavior of an ERC721 and maintain correct NFT ownerships. However, the underlying business logic makes it such that _new edge-cases not present in the default ERC721 implementation need to be handled with additional care_.\n\nThe idea for further optimization in the ERC721A contract came from realizing that the check for explicit ownership of the subsequent ID was only required to be performed a single time. Once the data is explicit, this check can be skipped, saving gas on all future transfers by removing a cold storage load. A boolean variable `nextTokenDataSet` packed together in the current (already warm) ownership slot could track whether the following data is explicit.\n\n### Development and Testing Approach\n\nAt the time, testing had become an integral part of my development process. I often wrote tests before the actual implementation guiding my process. The idea was that through sufficiently covering as many scenarios as possible in tests any coding mistake would eventually be uncovered. _However, this approach can backfire when the implementation nuances are disregarded_. For the ERC721A optimization, I ended up writing four unit tests in addition to Solmate's ERC721 unit and fuzz tests to validate that the ownerships remained consistent.\n\n```js\n /// Transfer last minted id.\n /// Ensure id beyond last is not written to.\n /// Ensure correct ownership.\n function test_transferFrom1() public {\n     // ...\n }\n\n /// Transfer last two minted IDs in batch.\n /// Ensure correct ownership.\n function test_transferFrom2() public {\n     // ...\n }\n\n /// Mint new tokens after previous transfers.\n /// Ensure correct ownership.\n function test_transferFrom3() public {\n     // ...\n }\n\n /// Transfer tokens in middle of batch.\n /// Ensure correct ownership.\n function test_transferFrom4() public {\n     // ...\n }\n```\n\nImagine my surprise when I was notified about a bug in the implementation.\n\n![Bug alert](/data/blog/2023/test-your-tests/bug-alert.png)\n![Bug alert](../../../public/data/blog/2023/test-your-tests/bug-alert.png)\n\nI was surprised how this was even possible with all those extra test scenarios. Eventually, in order to mitigate the bug, I ended up including an additional test-case which showed up as failing.\n\n```js\n/// Transfer first token in batch.\n/// Ensure correct ownership.\nfunction test_transferFrom5() public {\n    // ...\n}\n```\n\nThen I quickly applied a patch that would satisfy all the tests.\n\nImagine my shock when I discovered that I had inadvertently included another bug in my patch which the expanded tests did not catch. This realization led me to slow down and reflect on the shortcomings of my current approach.\n\n_What went wrong?_\n\nIt is hard to point a finger on what exactly had gone wrong and allowed a bug to slip through the cracks of all the tests. Was I supposed to create unit tests for every possible state? After tracking all relevant variables related to control flow and producing tests covering the cartesian product of all important states, I realized that such an approach is likely infeasible in most practical cases due to the high overhead and state explosion.\n\nI realized that there is no one single thing which I should have done differently given a limited budget on time. Nevertheless, a few things can be said. For one, I was **lacking structure and a systematic approach** when writing the tests. This is especially true when covering every possible state is infeasible. In my case, this lead to **missing important edge-cases** in the tests that were relevant to the optimization. Another thing that could be identified was that some tests were **testing multiple things at once** which pronounced the lack of structure.\n\nFinally another important factor was that I was **lacking expressive and meaningful fuzz tests** that could handle, multiple transfers with random IDs and arbitrary actors. Let's further analyze how testing shortcomings could be avoided in simpler examples.\n\n## Testing Shortcomings Exemplified\n\n### Testing WAD Conversions\n\nTo shed light on the concepts discussed, consider a simple exercise of writing a fuzz test for WAD unit conversions.\n\n```js\nuint256 constant WAD = 1e18;\n\nfunction fromWAD(uint256 a) pure returns (uint256 c) {\n    c = a / WAD;\n}\n\nfunction toWAD(uint256 a) pure returns (uint256 c) {\n    c = a * WAD;\n}\n\ncontract TestFromWAD_DONT is Test {\n    function test_toWAD_fromWAD(uint256 a) public {\n        if (a >= type(uint256).max / WAD) vm.expectRevert();\n\n        assertEq(fromWAD(toWAD(a)), a);\n    }\n}\n```\n\n```js\nRunning 1 test for test/TestFromWAD.sol:TestFromWAD_DONT\n[PASS] test_toWAD_fromWAD(uint256) (runs: 10000, μ: 1201, ~: 589)\nTest result: ok. 1 passed; 0 failed; 0 skipped; finished in 156.80ms\n```\n\nWhile the fuzz test passes, there is something wrong in this setup.\n\n<details>\n<summary><b>Spot the issue above! [spoiler]</b><br><br></summary>\n\nWhile the above test passed numerous fuzzing iterations, the test itself contains an incorrect boundary check which the fuzzer doesn't find. Specifically, the conversion should not revert for `a = type(uint256).max / WAD`.\n\n```diff\n-       if (a >= type(uint256).max / WAD) vm.expectRevert();\n+       if (a >  type(uint256).max / WAD) vm.expectRevert();\n```\n\nTo further illustrate this, we can create a unit test and pass the boundary point to our fuzz test.\n\n```js\nfunction test_toWAD_fromWAD_boundary() public {\n    uint256 a = type(uint256).max / WAD;\n\n    this.test_toWAD_fromWAD(a);\n}\n```\n\n```js\nRunning 2 tests for test/TestFromWAD.sol:TestFromWAD_DONT\n[PASS] test_toWAD_fromWAD(uint256) (runs: 10000, μ: 1222, ~: 589)\n[FAIL. Reason: call did not revert as expected] test_toWAD_fromWAD_boundary() (gas: 4093)\nTraces:\n  [4093] TestFromWAD_DONT::test_toWAD_fromWAD_boundary()\n    ├─ [3446] TestFromWAD_DONT::test_toWAD_fromWAD(115792089237316195423570985008687907853269984665640564039457 [1.157e59])\n    │   ├─ [0] VM::expectRevert()\n    │   │   └─ ← ()\n    │   └─ ← ()\n    └─ ← \"call did not revert as expected\"\n\nTest result: FAILED. 1 passed; 1 failed; 0 skipped; finished in 149.86ms\n```\n\nRerunning the tests, we see that the fuzz test passes, but the unit test which simply calls the fuzz test with a concrete value fails.\n\n_Why does the fuzzer not test the boundary value?_\n\nFrom the fuzzer's perspective, this value might appear as any other in the space of all possible `uint256` inputs. It is thus very unlikely that the fuzzer will input this value without some further knowledge about the system or without some static analysis of the code.\n\n</details>\n\nThis inaccuracy in the test exemplifies how an unclear precondition can not only obfuscate the true functionality but, in more severe scenarios, also mask vulnerabilities. For this reason, **it is important to know how your tool works**, their strengths and weaknesses, and any biases. It is also not uncommon for tools to contain quirks and bugs themselves. Leaning too heavily on a single tool or a type of test can be detrimental to the reliability of your testing results.\n\n_How could we have done better?_\n\nTo help the fuzzer do its work, we can restructure the test as follows:\n\n```js\ncontract TestFromWAD_DO is Test {\n    /// Testing for values where `toWAD(a)` shouldn't revert.\n    function test_toWAD_fromWAD(uint256 a) public {\n        a = bound(a, 0, type(uint256).max / WAD);\n\n        assertEq(fromWAD(toWAD(a)), a);\n    }\n\n    /// Testing for values where `toWAD(a)` should revert.\n    function test_toWAD_fromWAD_revert_Panic(uint256 a) public {\n        a = bound(a, type(uint256).max / WAD + 1, type(uint256).max);\n\n        vm.expectRevert(abi.encodeWithSignature(\"Panic(uint256)\", (0x11)));\n\n        assertEq(fromWAD(toWAD(a)), a);\n    }\n}\n```\n\n$$\nuint256 \\text{ domain}\n$$\n\n$$\n\\overset{\\raisebox{2pt}{\\color{green} passing}}{\n    ├\\underset{0}{─}────────────\\underset{\\hskip -1em \\frac {MAX} {WAD}}{─}┤\n}\n\\overset{\\raisebox{2pt} {\\color{red}reverting}}{\n    ├────\\underset{\\hskip -3.5em \\frac {MAX} {WAD} + 1}───────\\underset{\\hskip -0.4 em MAX }{───}┤\n}\n$$\n\nBy **dividing tests based on expected results** (success or failure), our testing approach becomes clearer and ensures both situations are sufficiently tested. Generally, we want to **avoid complex decision trees in our test**. A test should ideally be doing one thing only.\n\nThis also ensures that we are getting enough coverage for our specific test scenario. Before this refactor, it could have been possible for the happy path to be chosen 99% of the time and only 1% of the test cases covering the reverting path. In the new setup we ensure that both cases receive the same number of successful fuzz runs.\n\nAny **edge cases should be explicitly tested** in further unit (and fuzz) tests. However, in this simple case, we can rely on Foundry's `bound` function. This helps in testing near boundary conditions, since it adds a bias to the `min` and `max` values in `bound(var, min, max)`.\n\nFurther, testing for a specific error (`Panic(0x11)`) can identify unintentional coding errors. **Catching general (unspecified) reverts via `vm.expectRevert()` is strongly discouraged**. You should always be able to exactly anticipate your code's behavior and outcome—and specifically, in what way it will revert.\n\nAs a further improvement, one might argue that the calls `fromWAD(toWAD(a))` should be separated. Currently the expected revert could apply to both functions (since they are called internally). In order to pinpoint (and anticipate) the exact occurrence of the error, these functions should be called externally.\n\nIt's worth noting, however, that the current tests alone don't offer a clear picture of the actual behavior of the `toWAD` and `fromWAD` functions. Multiple functions satisfy the constraints set by the tests, as illustrated below:\n\n```js\nfunction fromWAD(uint256 a) pure returns (uint256 c) {\n    c = a + 1234;\n}\n\nfunction toWAD(uint256 a) pure returns (uint256 c) {\n    if (a > type(uint256).max / WAD) revert();\n    c = a - 1234;\n}\n```\n\nWithout getting lost in details, let's look at another example.\n\n### Testing WAD Multiplication\n\nWe just examined a case where a test contained a wrong precondition compared compared to the implementation. _What if, however, our test mirrored an incorrect precondition contained in the implementation itself?_\n\nThe next testing example showcases multiplication in `WAD` units using `unchecked` arithmetic for optimization purposes.\n\n```js\ncontract TestMulWAD_DONT is Test {\n    function mulWAD(uint256 a, uint256 b) public pure returns (uint256 c) {\n        unchecked {\n            if (b == 0 || a > type(uint256).max / b) revert Overflow();\n\n            c = a * b / WAD;\n        }\n    }\n\n    function test_mulWAD(uint256 a, uint256 b) public {\n        if (b == 0 || a > type(uint256).max / b) vm.expectRevert();\n\n        uint256 c = mulWAD(a, b);\n\n        assertEq(c, a * b / WAD);\n    }\n}\n```\n\nAgain, if we give the fuzzer a go at this test, it will report that it passes for all cases.\n\n```js\nRunning 1 test for test/TestMulWAD.sol:TestMulWAD_DONT\n[PASS] test_mulWAD(uint256,uint256) (runs: 10000, μ: 1698, ~: 732)\nTest result: ok. 1 passed; 0 failed; 0 skipped; finished in 158.97ms\n```\n\n<details>\n<summary><b>Spot the issue above! [spoiler]</b><br><br></summary>\n\nThe issue in this example arises from the incorrect precondition mirrored in the test. The boolean OR operator `||` is accidentally used instead of an AND operator `&&`.\n\n```diff\n    function mulWAD(uint256 a, uint256 b) public pure returns (uint256 c) {\n        unchecked {\n-           if (b == 0 || a > type(uint256).max / b) revert Overflow();\n+           if (b != 0 && a > type(uint256).max / b) revert Overflow();\n        }\n    }\n\n    function test_mulWAD(uint256 a, uint256 b) public {\n-       if (b == 0 || a > type(uint256).max / b) vm.expectRevert();\n+       if (b != 0 && a > type(uint256).max / b) vm.expectRevert();\n    }\n```\n\nA mistake like this could likely have been introduced by manually copying the flawed precondition for overflow to the test. Such a case, can not be detected by the fuzzer.\n\n</details>\n\n_How could we have done better to avoid the such a failure case?_\n\n### Exploring Various Testing Strategies\n\nAs we have just seen in the previous example, **solely relying on a single form of testing can lead to brittle results**. Including a simple _unit test_ for known values, e.g. `mulWAD(0, 0)` could have uncovered the incorrect check.\n\n```js\ncontract TestMulWAD_DO is Test {\n    /// Unit test\n    function test_mulWAD_unit() public {\n        uint256 a = 0;\n        uint256 b = 0;\n        uint256 c = mulWAD(a, b);\n\n        assertEq(c, 0);\n    }\n}\n```\n\nIt never hurts to cover the implementation behavior with multiple unit tests and **additional adjacent property/fuzz tests**. For example, checking that our function satisfies the commutative property would have uncovered the mistake in the code as well.\n\n```js\ncontract TestMulWAD_DO is Test {\n    // Fuzz test commutative property\n    function test_mulWAD_fuzz_commutative(uint256 a, uint256 b) public {\n        // If `a * b = c` doesn't overflow...\n        try this.mulWAD(a, b) returns (uint256 c) {\n            // then `b * a = c`.\n            assertEq(c, mulWAD(b, a));\n        } catch {}\n    }\n}\n```\n\nAnother point to be mindful of is that when code from the implementation is reused in a test, we rely on its correctness to be able to trust the outcome of the test. In this example an incorrect pre-condition was copied to the test, but we could also make the same mistake when we re-use function calls which evaluate a pre-condition.\n\nThis is why it's important to **approach testing from multiple angles**. In the case of the overflow check, we could have also re-implemented the overflow logic in another manner.\n\n```js\ncontract TestMulWAD_DO is Test {\n    /// Fuzz test\n    function test_mulWAD_fuzz(uint256 a, uint256 b) public {\n        unchecked {\n            // Compute `c = a * b` while ignoring overflow.\n            uint256 c = a * b;\n\n            // If we can't reconstruct `b = c / a`, then we lost some bits.\n            if (a != 0 && b != c / a) vm.expectRevert(Overflow.selector);\n\n            assertEq(mulWAD(a, b), c);\n        }\n    }\n}\n```\n\nWhat's more, if we have a reference implementation at hand, we can also **use fuzzing to detect any discrepancies by differentially testing** the behavior of two implementations against each other.\n\n```js\ncontract TestMulWAD_DO is Test {\n    function mulWADNative(uint256 a, uint256 b) public pure returns (uint256 c) {\n        c = a * b / WAD;\n    }\n\n    /// Differential test\n    function test_mulWAD_differential(uint256 a, uint256 b) public {\n        assertEqCall(\n            address(this),\n            abi.encodeCall(this.mulWAD, (a, b)),\n            abi.encodeCall(this.mulWADNative, (a, b))\n        );\n    }\n}\n```\n\nUsing the `ffi` (foreign function interface) cheat code we can also test Solidity code against non-Solidity implementations adding another safety layer.\n\n**Redundancy is key when aiming to create a robust testing suite**. Testing multiple properties through various strategies and techniques improves the confidence and soundness of the testing outcomes.\n\n## Offensive Testing\n\nSo far we have mostly been focusing on validating properties of our system as robustly as possible to strengthen behavioral guarantees. This could be termed as 'defensive testing', where expected behavior is solidified and validated. However, sometimes, the more revealing approach is the opposite—aiming to invalidate or disprove properties we suspect might not hold. This is what we could term as the 'offensive' side of testing.\n\n### Case Study: Primitive Finance - Hyper\n\nIn January 2023 Trail of Bits conducted a security review of 'Primitive Finance - Hyper AMM' (renamed to 'Portfolio'). The [report is publicly available](https://github.com/trailofbits/publications/blob/master/reviews/2023-03-primitive-securityreview.pdf) and Primitive has given permission to highlight the testing that was performed during our review of their system.\n\nAs an overview, a few notable characteristics of the system were:\n\n- CFMM (Constant Function Market Maker) protocol incorporating time-dependent curves, potentially enabling options trading.\n- Internal accounting system for pool balances along with a batch swapping functionality.\n- It heavily employs assembly language and intricate function approximations (gauss `pdf` and `ierfc`) in its dependency `solstat`. Additionally, it showcases inconsistent rounding behavior.\n\n### Initial Testing Strategy\n\nIn the early stages of the audit, a natural inclination was to employ fuzz testing. The hypothesis was simple: _Can one exploit inaccuracies in the pool's state (parameters, reserves, etc.) to achieve favorable swaps?_ The goal was to determine if repeated swaps could lead to value extraction.\n\nBelow is an excerpt from an initial test attempt:\n\n```js\nfunction test_swap(\n    bool sellAsset,\n    uint128 input,\n    uint128 tempOutput,\n    uint256 liquidity,\n    uint256 volatility,\n    uint256 duration,\n    uint128 stkPrice,\n    uint128 price\n) public {\n    // Alice create's an honest pool with liquidity.\n    createPoolAndAllocateLiquidity(alice, 100, 10, 2e18, 1e18, 1_000_000 * 1e18);\n\n    // Bob create's a malicious pool with liquidity.\n    createPoolAndAllocateLiquidity(bob, volatility, duration, stkPrice, price, liquidity);\n\n    // Bob tries to extract tokens by swapping back and forth.\n    uint128 targetValue = 1e18;\n\n    // Swap input X -> tempOutput Y -> output X.\n    bool success = swapBackAndForth(sellAsset, input, tempOutput, input + targetValue);\n    if (!success) return;\n\n    // Bob withdraws all of his assets.\n    withdrawAllAssets(bob);\n\n    int256 assetGain = int256(asset.balanceOf(bob)) - int256(INITIAL_BALANCE);\n    int256 quoteGain = int256(quote.balanceOf(bob)) - int256(INITIAL_BALANCE);\n\n    // Bob should gain tokens.\n    if (assetGain < 0 || quoteGain < 0) return;\n\n    // Log the balance gain.\n    console.log(\"Asset gain\", vm.toString(assetGain));\n    console.log(\"Quote gain\", vm.toString(quoteGain));\n\n    // Report the test case.\n    fail();\n}\n```\n\nThis initial approach led to repeated failures when setting up pools, allocating liquidity or when performing a swap.\n\n```js\nTraces:\n  [128911] self::test_swap(false, 0, 0, 0, 0, 0, 0, 0)\n    ├─ [0] VM::startPrank(alice: [0x00000000000000000000000000000000000A11cE])\n    │   └─ ← ()\n    ├─ [113457] HYPER::aa012c0c(5991a2df15a8f6a256d3ec51e99254cd3fb576a9f62849f9a0b5bf2913b396098f7c7019b51a820a)\n    │   ├─ emit CreatePair(pairId: 1, asset: ASSET: [0x5991A2dF15A8F6A256D3Ec51E99254Cd3fb576A9], quote: QUOTE: [0xF62849F9A0B5Bf2913b396098F7c7019b51A820a], decimalsAsset: 18, decimalsQuote: 18)\n    │   └─ ← ()\n    └─ ← \"UNDEFINED\"\n```\n\n```js\nTraces:\n  [306133] self::test_swap(false, 0, 0, 170141183460469231731687303715884105728 [1.701e38], 0, 0, 0, 0)\n    ├─ [0] VM::startPrank(alice: [0x00000000000000000000000000000000000A11cE])\n    │   └─ ← ()\n    ├─ [113457] HYPER::aa012c0c(5991a2df15a8f6a256d3ec51e99254cd3fb576a9f62849f9a0b5bf2913b396098f7c7019b51a820a)\n    │   ├─ emit CreatePair(pairId: 1, asset: ASSET: [0x5991A2dF15A8F6A256D3Ec51E99254Cd3fb576A9], quote: QUOTE: [0xF62849F9A0B5Bf2913b396098F7c7019b51A820a], decimalsAsset: 18, decimalsQuote: 18)\n    ...\n    ├─ [5912] HYPER::allocate(1103806595073 [1.103e12], 170141183460469231731687303715884105728 [1.701e38])\n    │   └─ ← \"EvmError: Revert\"\n    └─ ← \"EvmError: Revert\"\n```\n\nIn order to not spend too much time on a single thesis during the review, these reverting function calls were handled using `try ... catch`—simply discarding any error case. This way, the fuzzer could be allowed to continue to search for a combination of inputs that would invalidate the invariant while allowing us to continue the manual review of the code.\n\nThe fuzzer continued for nearly a week without presenting any exploit. **It would be easy, albeit premature, to interpret this as a sign of robustness against the hypothesized exploit.**\n\nIn order to make the fuzzer more effective at its task, it required further guidance when selecting the parameters increasing the amount of valid test cases. Ideally, **every single test run should produce valid parameters**. This is achieved by properly specifying the bounds on the parameters. However, **it's equally important not to discard any valid parameter combination**, as otherwise certain edge-cases which might be required for a successful exploit might be discarded.\n\nDefining tight bounds might require precisely reconstructing of the protocol's pre-conditions. Sometimes these conditions might be explicitly stated, and sometimes they might be complex to evaluate, and the bounds of one parameter can depend on another.\n\n```js\nfunction test_swap(/* ... */) public {\n    // Properly bound pool parameters.\n    duration = bound(duration, 1, 500);\n    volatility = bound(volatility, 100, 25_000);\n    stkPrice = uint128(bound((stkPrice), 1, type(uint128).max));\n    // Prevent lnWad's \"UNDEFINED\" when `price * 1e18 / stkPrice == 0`.\n    price = uint128(\n        bound(\n            uint256(price),\n            (Price.computePriceWithTick(Price.computeTickWithPrice(stkPrice) + 1) + 1e18 - 1) / 1e18,\n            type(uint128).max\n        )\n    );\n\n    // Alice create's a pool with liquidity.\n    // ...\n}\n```\n\n```js\nRunning 1 test for test/foundry/TestSwapPoc.t.sol:TestHyperSwapPoc\n[PASS] test_swap(bool,uint128,uint128,uint256,uint256,uint256,uint128,uint128) (runs: 10000, μ: 723780, ~: 794869)\nTest result: ok. 1 passed; 0 failed; 0 skipped; finished in 10.23s\n```\n\nAfter addressing all transaction reverts and properly bounding all parameters, all test runs should now complete.\n\nHowever, Bob has yet to make a profitable trade.\n\n### Refining the Testing Strategy\n\nA good way to test whether the test actually makes sense is by _testing it_. A well-written test should be able to immediately identify the case when an invariant is removed manually.\n\n```js\nliveInvariantWad = liveInvariantWad.scaleFromWadDownSigned(pool.pair.decimalsQuote); // invariant is denominated in quote token.\nnextInvariantWad = nextInvariantWad.scaleFromWadDownSigned(pool.pair.decimalsQuote);\nif (nextInvariantWad < liveInvariantWad) revert InvalidInvariant(liveInvariantWad, nextInvariantWad);\n```\n\nThis code enforces the invariant in the protocol. Generally, an invariant like this should guarantee that, under no circumstances, there's a possibility of a profitable trade when swapping back and forth. However, as the protocol invariant was uses the newly computed price and pool balances—also derived from the price—doubts whether this invariant was truly enforcing the desired properties emerged.\n\nBy removing the invariant check above, we should immediately be able to see whether the fuzzer is capable of finding inputs that invalidate our swap test case. Once our setup is validated, we can go further and remove any swap fees while re-introducing the invariant. If a failing test case can be found without swap fees, this is already a big hint that there might be some rounding, overflow or precision loss issue in the protocol that could be exploited.\n\n**We test our hypothesis by first making the conditions as ideal as possible for an attacker and then trim the capabilities back down to pinpoint the exact conditions required for an exploit.**\n\nA significant discovery came after questioning further assumptions made in the test setup. The assumption that Bob must gain a profit on both tokens is unnecessarily strict. Usually, when swapping `X quote -> Y asset -> Z quote`, the amount of Asset tokens required should be zero. This is because the second swap's input is simply defined as the first swaps output `Y asset`.\n\nNormally, this would mean that we are not required to provide any Asset tokens. However, in this particular system, due to the rounding logic in the protocol, often a minimum of `1` (wei) Asset token is required in the process of swapping back and forth. The test could be adapted to allow for one token to be lost, or simply removing\n\n```js\n// Bob should gain tokens.\nif (assetGain < -1 || quoteGain < -1) return;\n```\n\nFurthermore, Hyper's batched instruction processing allowed to execute more efficient swaps.\n\n```js\nfunction swapBackAndForth(bool sellAsset, uint128 input, uint128 output1, uint128 output2)\n    internal\n    returns (bool success)\n{\n    // Encode batch instructions.\n    bytes[] memory instructions = new bytes[](2);\n\n    instructions[0] = ProcessingLib.encodeSwap(0, poolId, 0, input, 0, output1, sellAsset ? 0 : 1);\n    instructions[1] = ProcessingLib.encodeSwap(0, poolId, 0, output1, 0, output2, sellAsset ? 1 : 0);\n\n    bytes memory data = ProcessingLib.encodeJumpInstruction(instructions);\n\n    (success,) = address(hyper).call(data);\n}\n```\n\nA last discovery was made when it was observed that the exploit would only be possible if the attacker was **not** the pool controller. The original idea was that the pool controller would receive a favorable fee compared to the standard swap fee. However, since the pool controller's fee is charged in Ether independently of the Asset and Quote tokens (but dependent on the pool's liquidity) this created a scenario, where the exploit was not possible.\n\nThis highlighted the **significance of questioning assumptions in tests**. The Primitive Hyper swap case offers several insights that can be applied to offensive testing:\n\n1. **Establish Precise Variable Bounds**: Address potential reverts by ensuring that all variables are bounded correctly. Ensure that the number of discarded test cases remains at a minimum.\n2. **Augment Coverage Insight**: Introduce sanity checks to assess the feasibility of the test hypothesis. Considering testing the setup by removing the protocol invariant checks.\n3. **Challenge Every Assumption**: The audit revealed how certain assumptions, like the fixed priority fee required for the exploit, or the prerequisite of token approvals, can skew results and lead to premature conclusions.\n\nIn essence, offensive testing requires a dynamic and persistent approach. It's not just about confirming the robustness of a system but actively seeking potential cracks. Only by repeatedly challenging our understanding and assumptions can we hope to uncover hidden vulnerabilities.\n\n## Conclusions and Takeaways\n\n1. **Treat your tests as production code**\n   - Tests can also contain bugs.\n   - Risks: Faulty tests can lead to misleading outcomes, false confidence and mask bugs.\n2. **Understand limitations of testing and tooling**\n   - Recognize flawed or insignificant tests.\n   - Tests can only go so far in reproducing realistic scenarios.\n   - Tools can have biases or bugs, or might not be able to cover all significant input space.\n3. **Explore different testing strategies and techniques**\n   - Incorporate unit, fuzz, integration tests.\n   - Test multiple properties and different angles.\n   - Add redundancy.\n4. **Examine assumptions, preconditions, and conclusions of tests**\n   - Check for hidden assumptions or inaccurate preconditions: e.g. actors behaving differently in a realistic scenario, assuming well-formed inputs, e.g. sorted tokens, bytes data encoded correctly.\n   - Do tested properties hold for all significant cases?\n5. **Test your tests**\n   - Create meta-tests.\n   - Introduce deliberate bugs to see if tests detect them.\n   - Apply mutation testing.\n\nQuestions to be asking yourself when reviewing your tests:\n\n- How can we ensure that our tests are sufficiently covering critical parts of the code base?\n- How can we measure the efficacy and robustness of our tests?\n- What kind of guarantees does the passing of a test give and under what circumstances are these valid?\n\n## Extra Material: Additional Testing Shortcomings\n\nThis section contains additional scenarios highlighting shortcomings in testing that would have made the main article too lengthy.\n\n### Testing Helper Functions\n\nThinking back to the shortcomings of tests in the ERC721A improvement example, we saw that expressive fuzz tests were absent (e.g. allowing multiple ID transfers).The next example examines a quick and dirty workaround that aims to cover that case.\n\nIn retrospect, the ERC721A improvement case highlighted a significant shortfall: the absence of expressive fuzz tests, including the ability to transfer multiple IDs. The next example explores a quick and practical fuzz test to address this deficiency.\n\nWe'll make use of a testing helper library called `random`, which will be used to generate a new pseudo random `uint256` number when calling `random.next()`. The library is seeded by calling `random.seed()`.\n\n```js\nlibrary random {\n    bytes32 constant RANDOM_SEED_SLOT = keccak256(\"random.seed.slot\");\n\n    /// @notice Seed randomness.\n    function seed(uint256 randomSeed) internal {\n        assembly {\n            // Store random seed.\n            sstore(RANDOM_SEED_SLOT, randomSeed)\n        }\n    }\n\n    /// @notice Generates a next random number by hashing\n    ///         the current seed value stored in `RANDOM_SEED_SLOT`.\n    function next() internal returns (uint256 nextRandom) {\n        assembly {\n            // Load random seed from storage slot.\n            let r := sload(RANDOM_SEED_SLOT)\n            // Compute keccak256 hash of seed.\n            mstore(0, r)\n            nextRandom := keccak256(0, 0x20)\n            // Update random seed.\n            sstore(RANDOM_SEED_SLOT, r)\n        }\n    }\n}\n```\n\nUsing this library we can quickly implement a function that will test `10` token transfers for all `30` minted IDs.\n\n```js\ncontract TestRandom_DONT is Test {\n    uint256 constant NUM_TRANSFERS = 10;\n    uint256 constant NUM_IDS = 30;\n\n    function test_transfer(uint256 seed) public {\n        // Seed randomness.\n        random.seed(seed);\n\n        // Create token and mint to `address(this)`.\n        ERC721 token = new ERC721A(NUM_IDS);\n\n        // Set all local token ownerships to `address(this)`.\n        address[NUM_IDS] memory owners;\n        for (uint256 i; i < NUM_IDS; i++) {\n            owners[i] = address(this);\n        }\n\n        for (uint256 i; i < NUM_TRANSFERS; i++) {\n            // Select a random id `nextId` to\n            // transfer from `currOwner` to a randomly\n            // selected `nextOwner`.\n            uint256 nextId = random.next() % NUM_IDS;\n            address nextOwner = address(uint160(random.next()));\n            address currOwner = owners[nextId];\n\n            // Transfer `nextId` from `currOwner` to `nextOwner`.\n            vm.prank(currOwner);\n            token.transferFrom(currOwner, nextOwner, nextId);\n\n            // Update local ownership data.\n            owners[nextId] = nextOwner;\n        }\n\n        // Validate final token ownerships.\n        for (uint256 i; i < NUM_IDS; i++) {\n            assertEq(owners[i], token.ownerOf(i));\n        }\n    }\n}\n```\n\nThrough analyzing this code, we can start out by highlighting some _hidden assumptions contained in this test that might not reflect real world conditions_ and are limitations of the setup:\n\n1. All `30` tokens are minted in a single batch to a single owner.\n   1. There are no successive mints to multiple (or the same) owners.\n   2. There is no mint of variable size (e.g. `0`, `1`, ...).\n   3. There is no further mint after IDs have been transferred.\n2. The `nextId` is selected uniformly (low edge-case probability).\n   1. Selecting previous `ID` or `ID + 1` is low.\n   2. Selecting `ID = 0` or `ID = 30` is low.\n3. The `nextOwner` is selected uniformly.\n   1. The probability that the same or a previous owner is selected is negligible.\n   2. The probability that the zero address is selected is negligible.\n\nDespite all these shortcomings and biases, this fuzz test should still be able to catch the missing edge case related to the ownership bug in the ERC721A implementation from earlier. However, it won't _ever_ be able to find it. This test will pass no matter what random IDs and owners will be drawn. In fact, this test might pass for entirely degenerate ERC721A implementations as well..\n\n_What is wrong with the given test setup and what steps would you employ to improve the security guarantees?_\n\n<details>\n<summary><b>Spot the issue above! [spoiler]</b><br><br></summary>\n\nThe issue here is that the random seed is not updated to the new value. The old previous seed is simply stored again.\n\n```diff\n            let r := sload(RANDOM_SEED_SLOT)\n            // Compute keccak256 hash of seed.\n            mstore(0, r)\n            nextRandom := keccak256(0, 0x20)\n            // Update random seed.\n-           sstore(RANDOM_SEED_SLOT, r)\n+           sstore(RANDOM_SEED_SLOT, nextRandom)\n```\n\nThis results in poor randomness. This library always returns the same random value, the same `nextTokenId` and the same `nextOwner`!\n\n_How could the mistake in the helper function have been discovered?_\n\nThe helper function is being used without any guarantees whatsoever. We require minimal tests for our helper functions to ensure that every test being built on top of it is working reliably.\n\n```js\ncontract TestRandom_DO is Test {\n    uint256 constant NUM = 30;\n\n    /// Test seeding the `random` library.\n    function test_seed(uint256 seed1, uint256 seed2) public {\n        // Require the seeds to differ.\n        vm.assume(seed1 != seed2);\n\n        // Store the first `NUM` random numbers\n        // produced by the first seed.\n        uint256[NUM] memory randomNumbers;\n\n        random.seed(seed1);\n\n        for (uint256 i; i < NUM; i++) {\n            randomNumbers[i] = random.next();\n        }\n\n        // Assert the random numbers produced\n        // by the second seed differ.\n        random.seed(seed2);\n\n        for (uint256 i; i < NUM; i++) {\n            assertTrue(randomNumbers[i] != random.next());\n        }\n\n        // Assert that resetting to the old\n        // seed produces the same random sequence.\n        random.seed(seed1);\n\n        for (uint256 i; i < NUM; i++) {\n            assertEq(randomNumbers[i], random.next());\n        }\n    }\n\n    /// @notice Marks a random number as seen.\n    mapping(uint256 => bool) randomNumberSeen;\n\n    /// Test the random numbers returned by the library do not repeat.\n    function test_next(uint256 seed) public {\n        random.seed(seed);\n\n        for (uint256 i; i < NUM; i++) {\n            uint256 r = random.next();\n\n            // The probability of r ∈ [0, 100_000]\n            // should be negligible.\n            assertTrue(r > 100_000);\n            // The random number should be unseen.\n            assertEq(randomNumberSeen[r], false);\n            // Mark number as seen.\n            randomNumberSeen[r] = true;\n        }\n    }\n}\n```\n\nTesting the `random` library's `seed` and `next` functions in a couple fuzz tests would have detected the issue related to the bad randomness.\n\nThat's why it's important to **test your helper functions** as well. Building a robust structure of tests requires a solid foundation. _There is no cake without a base!_\n\n</details>\n\n### Tool Quirks\n\nLet's revisit the notion that it's important to understand how to use your tool, its biases and quirks. Below is a function `MerkleLib.zeros` that returns pre-computed roots for empty subtrees of a given depth. These have been computed beforehand and are being validated and fixed through a unit test that checks all values.\n\n```js\nuint256 constant DEPTH = 4;\n\nlibrary MerkleLib {\n    error InvalidZerosLevel();\n\n    /// @notice Returns pre-computed zero sub-tree root of depth `level`.\n    ///         Each sub-tree root is computed by:\n    ///             root(0) := keccak256(enc(0, 0))\n    ///             root(N) := keccak256(enc(root(N-1), root(N-1)))\n    function zeros(uint256 level) internal pure returns (bytes32) {\n        if (level == 0) return 0x2098f5fb9e239eab3ceac3f27b81e481dc3124d55ffed523a839ee8446b64864;\n        if (level == 1) return 0x1069673dcdb12263df301a6ff584a7ec261a44cb9dc68df067a4774460b1f1e1;\n        if (level == 2) return 0x18f43331537ee2af2e3d758d50f72106467c6eea50371dd528d57eb2b856d238;\n        if (level == 3) return 0x07f9d837cb17b0d36320ffe93ba52345f1b728571a568265caac97559dbc952a;\n        if (level == 4) return 0x2b94cf5e8746b3f5c9631f4c5df32907a699c58c94b2ad4d7b5cec1639183f55;\n        revert InvalidZerosLevel();\n    }\n}\n\ncontract TestMerkle_DONT is Test {\n    /// Test `MerkleLib.zeros` returns correct hash values.\n    function test_zeros() public {\n        bytes32 node;\n\n        for (uint256 i; i < DEPTH + 1; i++) {\n            node = keccak256(abi.encode(node, node));\n\n            assertEq(MerkleLib.zeros(i), node);\n        }\n\n        // Calling `MerkleLib.zeros(DEPTH + 1)` should revert.\n        vm.expectRevert(MerkleLib.InvalidZerosLevel.selector);\n        MerkleLib.zeros(DEPTH + 1);\n    }\n}\n```\n\nThe test `test_zeros` passes the unit test.\n\n```js\nRunning 1 test for test/TestMerkle.sol:TestMerkle_DONT\n[PASS] test_zeros() (gas: 46756)\nTest result: ok. 1 passed; 0 failed; 0 skipped; finished in 337.54µs\n```\n\n_What is wrong here?_\n\nIf you recall some of the advice previously given, then you might be able to rewrite the test in a safer way and spot the issue.\n\n<details>\n<summary><b>Spot the issue above! [spoiler]</b><br><br></summary>\n\nOnce we split up our test based on the expected outcomes (passing, reverting), we immediately see the issue at hand.\n\n```js\ncontract TestMerkle_DO is Test {\n    /// Test `MerkleLib.zeros` returns correct hash values.\n    function test_zeros() public {\n        bytes32 node;\n\n        for (uint256 i; i < DEPTH + 1; i++) {\n            node = keccak256(abi.encode(node, node));\n\n            assertEq(MerkleLib.zeros(i), node);\n        }\n    }\n\n    /// Calling `MerkleLib.zeros(DEPTH + 1)` should revert.\n    function test_zeros_revert_InvalidZerosLevel() public {\n        vm.expectRevert(MerkleLib.InvalidZerosLevel.selector);\n        MerkleLib.zeros(DEPTH + 1);\n    }\n}\n```\n\nThis produces the following output.\n\n```js\nRunning 2 tests for test/TestMerkle.sol:TestMerkle_DO\n[FAIL. Reason: Assertion failed.] test_zeros() (gas: 46138)\n[PASS] test_zeros_revert_InvalidZerosLevel() (gas: 3252)\nTest result: FAILED. 1 passed; 1 failed; 0 skipped; finished in 397.04µs\n\nRan 2 test suites: 2 tests passed, 1 failed, 0 skipped (3 total tests)\n```\n\nWe now see that the first part of the test was actually failing. All the pre-computed hashes have been calculated using the Poseidon hash instead of `keccak256` resulting in incorrect values.\n\n_Why does the test pass nonetheless?_\n\nThe reason the single test passes nonetheless when testing both things at once is due to a [quirk in Foundry](https://github.com/foundry-rs/foundry/issues/4832) related to using `vm.expectRevert` on internal functions. Failing `assert` statements are ignored if a revert is caught on an internal function.\n\n</details>\n\nRemember the advice from earlier that **a test should only be testing one thing** at a time. Again, this also makes the testing structure clearer and simpler.\n","title":"Test Your Tests!","date":"Oct 20, 2023","excerpt":"Writeup of the talks given at TrustX & Solidity Summit at Devconnect in Istanbul.","suptitle":"The Dos and Don'ts of testing"}},"__N_SSG":true}