{"pageProps":{"postData":{"slug":["blog","2023","test-your-tests"],"contentRaw":"\n> This is a writeup of the talks given at [TrustX](https://www.secureum.xyz/trustx/) and [Solidity Summit](https://soliditylang.org/summit/) at [Devconnect Istanbul 2023](https://devconnect.org/).\n\n> I am a security researcher and engineer at Trail of Bits.\n\n_In order to gain the most out of this article, it is advised that you actively try to discover the issue within the tests in the examples (hidden behind spoiler tags). Take a moment and consider what patterns might seem dangerous or brittle and how you could implement safer tests._\n\n**Table of Contents**\n\n- [The Role of Testing](#the-role-of-testing)\n- [Motivation Behind the Topic](#motivation-behind-the-topic)\n  - [Improving ERC721A](#improving-erc721a)\n  - [Development and Testing Approach](#development-and-testing-approach)\n- [Testing Shortcomings Exemplified](#testing-shortcomings-exemplified)\n  - [Testing WAD Conversions](#testing-wad-conversions)\n  - [Testing WAD Multiplication](#testing-wad-multiplication)\n  - [Exploring Various Testing Strategies](#exploring-various-testing-strategies)\n- [Offensive Testing](#offensive-testing)\n  - [Case Study: Primitive Finance - Hyper](#case-study-primitive-finance---hyper)\n  - [Initial Testing Strategy](#initial-testing-strategy)\n  - [Refining the Testing Strategy](#refining-the-testing-strategy)\n- [Conclusions and Takeaways](#conclusions-and-takeaways)\n- [Extra Material: Additional Testing Shortcomings](#extra-material-additional-testing-shortcomings)\n  - [Testing Helper Functions](#testing-helper-functions)\n  - [Tool Quirks](#tool-quirks)\n\nIn the rapidly evolving landscape of software development, **the importance of rigorous testing cannot be overstated**. This is especially true when dealing with mission-critical systems like blockchains with millions at risk. The reliability of tests are fundamental to ensuring the integrity and security of these systems. But, _how can we guarantee robustness of our tests, and how do we evaluate their quality and efficacy?_\n\n## The Role of Testing\n\n_Why do we test our software?_\n\nTests are primarily written with the goal of **identifying incorrect or unexpected behavior** of our software. Unit tests are great for validating that our code will produce a certain outcome given an input. With the advancement of automated testing we are also able to **validate properties or invariants of our system** by simulating function calls with persistent state. Unit and fuzz tests can also be great for **ensuring that a piece of code adheres to its specification**. Tests for the ERC721 standard, for example, can be applied to an extension of the specification in order to uncover unexpected deviations from the requirements of the norm. What's more is that tests act as the protective layer for code functionality. A well-crafted test should **cement the anticipated code behavior**, safeguarding it against unexpected issues that might be introduced in future modifications.\n\nAlthough applying various testing methods and static analysis can vouch for your code's stability, it's important to keep in mind that **no test and testing method is infallible**. _Testing is an ongoing process of refinement, not a final endpoint!_ It is impossible to gain absolute certainty that your code will be bug-free. Tests may contain bugs themselves, fuzzers can be used ineffectively and even tools for formal verification can contain bugs or suffer from state explosion or bad heuristics. Constant attention and multiple testing methods are essential for ensuring software robustness.\n\n## Motivation Behind the Topic\n\nThis topic's inspiration stems from my own experiences and reflections on past shortcomings. Before becoming a security researcher, I paved my path as a developer. Every developer makes a mistake at some point. I wanted to learn from every single one of them. Each identified vulnerability led me to re-evaluate my approach—_how could I improve my development and testing process to prevent similar situations in the future?_\n\nA memorable experience in this regard remains when working to optimize the [ERC721A](https://www.erc721a.org/) contract.\n\n### Improving ERC721A\n\nERC721A works by leveraging batched token mints involving sequential IDs. The idea is to defer costly storage operations at a time of high network costs (during the mint) to a time of low network costs (for subsequent transfers). This is done by keeping the ownership data of IDs minted in sequence implicit instead of explicitly committing them to the storage one by one.\n\nWhen identifying the owner of an ID, if the ownership data is empty (uncommitted/implicit), then the owner is looked up in the previous slot.\n\n```\nBob mints 5 IDs:\n               tokenId\n                  V\n| X | Bob |    |    |    |    | Y |\n       ^\n      from\n\nBob transfers `tokenId` to Eve:\n\n| X | Bob |    | Eve |    |    | Y |\n                  ^\n                  to\n\nFollowing Ownership Commit:\n\n| X | Bob |    | Eve | Bob |    | Y |\n                        ^\n```\n\nUpon transfers, ERC721A checks whether the ownership data of the subsequent ID is explicit or implicit. In order to maintain consistent ownership data, it must be set explicitly.\n\nThe idea for further optimization in the ERC721A contract came from realizing that the check for explicit ownership of the subsequent ID was only required to be performed a single time. Once the data is explicit, this check can be skipped, saving gas on all future transfers by removing a cold storage load. A boolean variable `nextTokenDataSet` packed together in the current (already warm) ownership slot could track whether the following data is explicit.\n\n### Development and Testing Approach\n\nAt the time, testing had become an integral part of my development process. I often wrote tests before the actual implementation guiding my process. The idea was that through sufficiently covering as many scenarios as possible in tests any coding mistake would eventually be uncovered. However, this approach can backfire when any implementation nuances are disregarded. For the ERC721A optimization, I ended up writing four unit tests in addition to Solmate's ERC721 unit and fuzz tests to validate that the ownerships remained consistent.\n\n```js\n /// Transfer last minted id.\n /// Ensure id beyond last is not written to.\n /// Ensure correct ownership.\n function test_transferFrom1() public {\n     // ...\n }\n\n /// Transfer last two minted IDs in batch.\n /// Ensure correct ownership.\n function test_transferFrom2() public {\n     // ...\n }\n\n /// Mint new tokens after previous transfers.\n /// Ensure correct ownership.\n function test_transferFrom3() public {\n     // ...\n }\n\n /// Transfer tokens in middle of batch.\n /// Ensure correct ownership.\n function test_transferFrom4() public {\n     // ...\n }\n```\n\nImagine my surprise when I was notified about a bug in the implementation.\n\n![Alt text](image.png)\n\nI was surprised how this was even possible with all those extra test scenarios. Eventually, in order to mitigate the bug, I ended up including an additional test-case which showed up as failing.\n\n```js\n    /// Transfer first token in batch.\n    /// Ensure correct ownership.\n    function test_transferFrom5() public {\n        // ...\n    }\n```\n\nThen I quickly applied a patch that would satisfy all the tests.\n\nImagine my shock when I discovered that I had inadvertently included another bug in my patch which the additional test did not catch. This realization led me to slow down and reflect on the shortcomings of my current approach.\n\n_What went wrong?_\n\nIt is hard to point a finger on what exactly had gone wrong and allowed a bug to slip through the cracks of all the tests. Was I supposed to create unit tests for every possible state? After tracking all relevant variables related to control flow and producing tests covering the cartesian product of all important states, I realized that such an approach is most likely infeasible in most cases due to the high overhead and state explosion.\n\nI realized that there is no one single thing which I should have done differently given a limited budget on time. Nevertheless, a few things can be said. For one, I was **lacking structure and a systematic approach** when writing the tests. This is especially true when covering every possible state is infeasible. In my case, this lead to **missing important edge-cases** in the tests that were relevant to the optimization. Another thing that could be identified was that some tests were **testing multiple things at once** which pronounced the lack of structure.\n\nFinally another important factor was that I was **lacking expressive and meaningful fuzz tests** that could handle, multiple transfers with random IDs and arbitrary actors.\n\nLet's further analyze how testing shortcomings could be avoided in simpler examples.\n\n## Testing Shortcomings Exemplified\n\n### Testing WAD Conversions\n\nTo shed light on the concepts discussed, consider a simple exercise of writing a fuzz test for WAD unit conversions.\n\n```js\nuint256 constant WAD = 1e18;\n\nfunction fromWAD(uint256 a) pure returns (uint256 c) {\n    c = a / WAD;\n}\n\nfunction toWAD(uint256 a) pure returns (uint256 c) {\n    c = a * WAD;\n}\n\ncontract TestFromWAD_DONT is Test {\n    function test_toWAD_fromWAD(uint256 a) public {\n        if (a >= type(uint256).max / WAD) vm.expectRevert();\n\n        assertEq(fromWAD(toWAD(a)), a);\n    }\n}\n```\n\n```js\nRunning 1 test for test/TestFromWAD.sol:TestFromWAD_DONT\n[PASS] test_toWAD_fromWAD(uint256) (runs: 10000, μ: 1201, ~: 589)\nTest result: ok. 1 passed; 0 failed; 0 skipped; finished in 156.80ms\n```\n\n<details>\n<summary><b>Spot the issue above! [spoiler]</b><br><br></summary>\n\nWhile the above test passed numerous fuzzing iterations, the test itself contains an incorrect boundary check which the fuzzer doesn't find. Specifically, the conversion should not revert for `a = type(uint256).max / WAD`.\n\n```diff\n-       if (a >= type(uint256).max / WAD) vm.expectRevert();\n+       if (a >  type(uint256).max / WAD) vm.expectRevert();\n```\n\nTo further illustrate this, we can create a unit test and pass the boundary point to our fuzz test.\n\n```js\nfunction test_toWAD_fromWAD_boundary() public {\n    uint256 a = type(uint256).max / WAD;\n\n    this.test_toWAD_fromWAD(a);\n}\n```\n\n```js\nRunning 2 tests for test/TestFromWAD.sol:TestFromWAD_DONT\n[PASS] test_toWAD_fromWAD(uint256) (runs: 10000, μ: 1222, ~: 589)\n[FAIL. Reason: call did not revert as expected] test_toWAD_fromWAD_boundary() (gas: 4093)\nTraces:\n  [4093] TestFromWAD_DONT::test_toWAD_fromWAD_boundary()\n    ├─ [3446] TestFromWAD_DONT::test_toWAD_fromWAD(115792089237316195423570985008687907853269984665640564039457 [1.157e59])\n    │   ├─ [0] VM::expectRevert()\n    │   │   └─ ← ()\n    │   └─ ← ()\n    └─ ← \"call did not revert as expected\"\n\nTest result: FAILED. 1 passed; 1 failed; 0 skipped; finished in 149.86ms\n```\n\nRerunning the tests, we see that the fuzz test passes, but the unit test which simply calls the fuzz test with a concrete value fails.\n\n_Why does the fuzzer not test the boundary value?_\n\nFrom the fuzzer's perspective, this value might appear as any other in the space of all possible `uint256` inputs. It is thus very unlikely that the fuzzer will input this value without some further knowledge about the system or without some static analysis of the code.\n\n</details>\n\nThis inaccuracy in the test exemplifies how an unclear precondition can not only obfuscate the true functionality but, in more severe scenarios, also mask vulnerabilities. For this reason, **it is important to know how your tool works**, its strengths and weaknesses, and any biases. Its also not uncommon for tools to contain quirks and bugs themselves. Leaning too heavily on a single tool or a type of test can be detrimental to the reliability of your testing results.\n\n_How could we have done better?_\n\nTo help the fuzzer do its work, we can restructure the test as follows:\n\n```js\ncontract TestFromWAD_DO is Test {\n    /// Testing for values where `toWAD(a)` shouldn't revert.\n    function test_toWAD_fromWAD(uint256 a) public {\n        a = bound(a, 0, type(uint256).max / WAD);\n\n        assertEq(fromWAD(toWAD(a)), a);\n    }\n\n    /// Testing for values where `toWAD(a)` should revert.\n    function test_toWAD_fromWAD_revert_Panic(uint256 a) public {\n        a = bound(a, type(uint256).max / WAD + 1, type(uint256).max);\n\n        vm.expectRevert(abi.encodeWithSignature(\"Panic(uint256)\", (0x11)));\n\n        assertEq(fromWAD(toWAD(a)), a);\n    }\n}\n```\n\n$$uint256 \\text{ domain}$$\n\n$$\n\\overset{\\raisebox{2pt}{\\color{green} passing}}{\n    ├\\underset{0}{─}────────────\\underset{\\hskip -1em \\frac {MAX} {WAD}}{─}┤\n}\n\\overset{\\raisebox{2pt} {\\color{red}reverting}}{\n    ├────\\underset{\\hskip -3.5em \\frac {MAX} {WAD} + 1}───────\\underset{\\hskip -0.4 em MAX }{───}┤\n}\n$$\n\nBy **dividing tests based on expected results** (success or failure), our testing approach becomes clearer and ensures both situations are sufficiently tested. Generally, we want to **avoid complex decision trees in our test**. A test should ideally be doing one thing only. This also ensures that we are getting enough coverage for our specific test scenario.\n\nAny **edge cases should be explicitly tested** in further unit (and fuzz) tests. However, in this simple case, using Foundry's `bound` function can aid in testing near boundary conditions.\n\nFurther, testing for a specific error (`Panic(0x11)`) can identify unintentional coding errors. **Catching general (unspecified) reverts via `vm.expectRevert()` is strongly discouraged**. You should always be able to predict your code's behavior and outcome--and specifically, how it will revert.\n\nAs a further improvement, one might argue that the calls `fromWAD(toWAD(a))` should be separated. Currently the expected revert could apply to both functions (since they are called internally). In order to specify the exact location of the error, these functions should be called externally.\n\nIt's worth noting, however, that the current tests alone don't offer a clear picture of the actual behavior of the `toWAD` and `fromWAD` functions. Multiple functions can satisfy the constraints set by the tests, as illustrated below:\n\n```js\nfunction fromWAD(uint256 a) pure returns (uint256 c) {\n    c = a + 1234;\n}\n\nfunction toWAD(uint256 a) pure returns (uint256 c) {\n    if (a > type(uint256).max / WAD) revert();\n    c = a - 1234;\n}\n```\n\nHowever, let's not get carried away with too many details and look at another example.\n\n### Testing WAD Multiplication\n\nWe just examined the case where there was a wrong precondition in a test compared to the implementation. _What if, however, our test reflected an incorrect precondition contained in the implementation itself?_\n\nThe next testing example showcases a multiplication in `WAD` units using `unchecked` arithmetic for optimization purposes.\n\n```js\ncontract TestMulWAD_DONT is Test {\n    function mulWAD(uint256 a, uint256 b) public pure returns (uint256 c) {\n        unchecked {\n            if (b == 0 || a > type(uint256).max / b) revert Overflow();\n\n            c = a * b / WAD;\n        }\n    }\n\n    function test_mulWAD(uint256 a, uint256 b) public {\n        if (b == 0 || a > type(uint256).max / b) vm.expectRevert();\n\n        uint256 c = mulWAD(a, b);\n\n        assertEq(c, a * b / WAD);\n    }\n}\n```\n\nAgain, if we give the fuzzer a go at this test, it will report that it passes for all cases.\n\n```js\nRunning 1 test for test/TestMulWAD.sol:TestMulWAD_DONT\n[PASS] test_mulWAD(uint256,uint256) (runs: 10000, μ: 1698, ~: 732)\nTest result: ok. 1 passed; 0 failed; 0 skipped; finished in 158.97ms\n```\n\n<details>\n<summary><b>Spot the issue above! [spoiler]</b><br><br></summary>\n\nThe issue in this example arises from the incorrect precondition mirrored in the test. The boolean OR operator `||` should have been an AND operator `&&`.\n\n```diff\n-           if (b == 0 || a > type(uint256).max / b) revert Overflow();\n+           if (b == 0 && a > type(uint256).max / b) revert Overflow();\n...\n-       if (b == 0 || a > type(uint256).max / b) vm.expectRevert();\n+       if (b == 0 && a > type(uint256).max / b) vm.expectRevert();\n```\n\nA mistake like this could likely have been introduced by manually copying the flawed precondition for overflow to the test. In this case, it is impossible to be detected by the fuzzer.\n\n</details>\n\n_How could we have done better to avoid the error in such a case?_\n\n### Exploring Various Testing Strategies\n\nAs we have just seen in the previous example, **solely relying on a single form of testing can lead to brittle results**. A simple **unit test** for `mulWAD(0, 0)` could have uncovered the incorrect check.\n\n```js\ncontract TestMulWAD_DO is Test {\n    /// Unit test\n    function test_mulWAD_unit() public {\n        uint256 a = 0;\n        uint256 b = 0;\n        uint256 c = mulWAD(a, b);\n\n        assertEq(c, 0);\n    }\n}\n```\n\nLikewise, it never hurts to **test for multiple adjacent properties** of our implementation through further fuzz tests. As an example, checking that our function is commutative would have uncovered the mistake in our code as well.\n\n```js\ncontract TestMulWAD_DO is Test {\n    // Fuzz test commutative property\n    function test_mulWAD_fuzz_commutative(uint256 a, uint256 b) public {\n        // If `a * b = c` doesn't overflow...\n        try this.mulWAD(a, b) returns (uint256 c) {\n            // then `b * a = c`.\n            assertEq(c, mulWAD(b, a));\n        } catch {}\n    }\n}\n```\n\nAnother thing to keep in mind is that the more code is copied from our implementation to the test, the less is actually tested. In this example an incorrect pre-condition was copied to the test, but we could also make the same mistake when we re-use function calls which evaluate a pre-condition.\n\nThis is why it's important to **approach testing from multiple angles**. In the case of the overflow check, we could have also re-implemented the overflow logic in another manner.\n\n```js\ncontract TestMulWAD_DO is Test {\n    /// Fuzz test\n    function test_mulWAD_fuzz(uint256 a, uint256 b) public {\n        unchecked {\n            // Compute `c = a * b` while ignoring overflow.\n            uint256 c = a * b;\n\n            // If we can't reconstruct `b = c / a`, then we lost some bits.\n            if (a != 0 && b != c / a) vm.expectRevert(Overflow.selector);\n\n            assertEq(mulWAD(a, b), c);\n        }\n    }\n}\n```\n\nWhat's more, if we have a reference implementation at hand, we can also **use fuzzing to detect any discrepancies by differentially testing** two implementations against each other.\n\n```js\ncontract TestMulWAD_DO is Test {\n    function mulWADNative(uint256 a, uint256 b) public pure returns (uint256 c) {\n        c = a * b / WAD;\n    }\n\n    /// Differential test\n    function test_mulWAD_differential(uint256 a, uint256 b) public {\n        assertEqCall(\n            address(this),\n            abi.encodeCall(this.mulWAD, (a, b)),\n            abi.encodeCall(this.mulWADNative, (a, b))\n        );\n    }\n}\n```\n\nUsing the `ffi` (foreign function interface) cheat code we can also test Solidity code against non Solidity implementations.\n\n**Redundancy is key when aiming to create a robust testing suite**. Testing multiple properties through various strategies and techniques can improve the soundness of your results.\n\n## Offensive Testing\n\nSo far we have mostly been focusing on validating properties of our system as robustly as possible to strengthen behavioral guarantees. This could be termed as \"defensive testing\", where expected behavior is solidified and validated. However, sometimes, the more revealing approach is the opposite—trying to invalidate or disprove properties we suspect might not hold. This is what we could term as the \"offensive\" side of testing.\n\n### Case Study: Primitive Finance - Hyper\n\nIn January 2023 Trail of Bits conducted an audit of 'Primitive Finance - Hyper AMM' (now called 'Portfolio'). The report is public and we have received permission to highlight the testing that was performed during the review.\n\nA few notable characteristics of the system were observed:\n\n- CFMM (Constant Function Market Maker) protocol incorporating time-dependent curves, potentially enabling options trading.\n- Internal accounting system for pool balances along with a batch swapping functionality.\n- It heavily employs assembly language and intricate function approximations (gauss `pdf` and `ierfc`) in its dependency `solstat`. Additionally, it showcases inconsistent rounding behavior.\n\n### Initial Testing Strategy\n\nIn the early stages of the audit, a natural inclination was to employ fuzz testing. The hypothesis was simple: _Can one exploit inaccuracies in the pool's state (parameters, reserves, etc.) to achieve favorable swaps?_ The goal was to determine if repeated swaps could lead to value extraction.\n\nBelow is an excerpt from an initial test attempt:\n\n```js\nfunction test_swap(\n    bool sellAsset,\n    uint128 input,\n    uint128 tempOutput,\n    uint256 liquidity,\n    uint256 volatility,\n    uint256 duration,\n    uint128 stkPrice,\n    uint128 price\n) public {\n    // Alice create's an honest pool with liquidity.\n    createPoolAndAllocateLiquidity(alice, 100, 10, 2e18, 1e18, 1_000_000 * 1e18);\n\n    // Bob create's a malicious pool with liquidity.\n    createPoolAndAllocateLiquidity(bob, volatility, duration, stkPrice, price, liquidity);\n\n    // Bob tries to extract tokens by swapping back and forth.\n    uint128 targetValue = 1e18;\n\n    // Swap input X -> tempOutput Y -> output X.\n    bool success = swapBackAndForth(sellAsset, input, tempOutput, input + targetValue);\n    if (!success) return;\n\n    // Bob withdraws all of his assets.\n    withdrawAllAssets(bob);\n\n    int256 assetGain = int256(asset.balanceOf(bob)) - int256(INITIAL_BALANCE);\n    int256 quoteGain = int256(quote.balanceOf(bob)) - int256(INITIAL_BALANCE);\n\n    // Bob should gain tokens.\n    if (assetGain < 0 || quoteGain < 0) return;\n\n    // Log the balance gain.\n    console.log(\"Asset gain\", vm.toString(assetGain));\n    console.log(\"Quote gain\", vm.toString(quoteGain));\n\n    // Report the test case.\n    fail();\n}\n```\n\nThis initial approach led to repeated failures when setting up pools, allocating liquidity or when performing a swap.\n\n```js\nTraces:\n  [128911] self::test_swap(false, 0, 0, 0, 0, 0, 0, 0)\n    ├─ [0] VM::startPrank(alice: [0x00000000000000000000000000000000000A11cE])\n    │   └─ ← ()\n    ├─ [113457] HYPER::aa012c0c(5991a2df15a8f6a256d3ec51e99254cd3fb576a9f62849f9a0b5bf2913b396098f7c7019b51a820a)\n    │   ├─ emit CreatePair(pairId: 1, asset: ASSET: [0x5991A2dF15A8F6A256D3Ec51E99254Cd3fb576A9], quote: QUOTE: [0xF62849F9A0B5Bf2913b396098F7c7019b51A820a], decimalsAsset: 18, decimalsQuote: 18)\n    │   └─ ← ()\n    └─ ← \"UNDEFINED\"\n```\n\n```js\nTraces:\n  [306133] self::test_swap(false, 0, 0, 170141183460469231731687303715884105728 [1.701e38], 0, 0, 0, 0)\n    ├─ [0] VM::startPrank(alice: [0x00000000000000000000000000000000000A11cE])\n    │   └─ ← ()\n    ├─ [113457] HYPER::aa012c0c(5991a2df15a8f6a256d3ec51e99254cd3fb576a9f62849f9a0b5bf2913b396098f7c7019b51a820a)\n    │   ├─ emit CreatePair(pairId: 1, asset: ASSET: [0x5991A2dF15A8F6A256D3Ec51E99254Cd3fb576A9], quote: QUOTE: [0xF62849F9A0B5Bf2913b396098F7c7019b51A820a], decimalsAsset: 18, decimalsQuote: 18)\n    ...\n    ├─ [5912] HYPER::allocate(1103806595073 [1.103e12], 170141183460469231731687303715884105728 [1.701e38])\n    │   └─ ← \"EvmError: Revert\"\n    └─ ← \"EvmError: Revert\"\n```\n\nIn order to not spend too much time on a single thesis during the review, these reverting function calls were handled using `try ... catch`, simply ignoring the error case. This way, the fuzzer could be allowed to continue to search for a combination of inputs that would invalidate the test case while further continuing the manual review of the code.\n\nThe fuzzer continued for nearly a week without presenting any exploit. **It would be easy, albeit premature, to interpret this as a sign of robustness against the hypothesized exploit.**\n\nIn order to make the fuzzer more effective at its work, it required further guidance when selecting the parameters increasing the amount of valid test cases.\nIdeally, **every single test run should produce valid parameters**. This is achieved by properly specifying the bounds on the parameters. However, **it's equally important not to discard any valid parameter combination**, as otherwise certain edge-cases which might be required for a successful exploit might be discarded.\n\nDefining tight bounds might require a precise reconstruction of the protocol's conditions. Sometimes these conditions might become complex to evaluate, especially when the bounds of one parameter depends on another.\n\n```js\nfunction test_swap(/* ... */) public {\n    // Properly bound pool parameters.\n    duration = bound(duration, 1, 500);\n    volatility = bound(volatility, 100, 25_000);\n    stkPrice = uint128(bound((stkPrice), 1, type(uint128).max));\n    // Prevent lnWad's \"UNDEFINED\" when `price * 1e18 / stkPrice == 0`.\n    price = uint128(\n        bound(\n            uint256(price),\n            (Price.computePriceWithTick(Price.computeTickWithPrice(stkPrice) + 1) + 1e18 - 1) / 1e18,\n            type(uint128).max\n        )\n    );\n\n    // Alice create's a pool with liquidity.\n    // ...\n}\n```\n\n```js\nRunning 1 test for test/foundry/TestSwapPoc.t.sol:TestHyperSwapPoc\n[PASS] test_swap(bool,uint128,uint128,uint256,uint256,uint256,uint128,uint128) (runs: 10000, μ: 723780, ~: 794869)\nTest result: ok. 1 passed; 0 failed; 0 skipped; finished in 10.23s\n```\n\nAfter addressing all transaction reverts and properly bounding all parameters, all test runs should now complete.\n\nHowever, Bob has yet to make a profitable trade.\n\n### Refining the Testing Strategy\n\nA good way to test whether the test actually makes sense is by testing it. A well-written test should be able to identify the case when an invariant is removed manually.\n\n```js\nliveInvariantWad = liveInvariantWad.scaleFromWadDownSigned(pool.pair.decimalsQuote); // invariant is denominated in quote token.\nnextInvariantWad = nextInvariantWad.scaleFromWadDownSigned(pool.pair.decimalsQuote);\nif (nextInvariantWad < liveInvariantWad) revert InvalidInvariant(liveInvariantWad, nextInvariantWad);\n```\n\nThis code enforces the invariant in the protocol. Generally, an invariant should guarantee that there is under no circumstance any possibility for a swap back and forth to execute a profitable trade. However, I had my doubts whether this invariant truly was enforcing this rule as the protocol invariant was being derived using the newly computed price and pool balances are all derived from the price.\n\nBy commenting the above invariant check, it should be possible to see whether the fuzzer even has the capability of finding inputs that invalidate our test.\n\nOnce our setup is validated, we can go further and pre-compute the maximum possible output amounts that satisfy the invariant. However, this time, we shall remove any swap fees to test our logic. If a failing test case can be found without the swap fees, this is already a big hint that there might be some rounding, overflow or precision loss issue in the protocol that could be exploited. We want to make the conditions as ideal as possible for an attacker and then trim the capabilities back down to see what conditions are necessary for an exploit.\n\nA significant discovery came after questioning further assumptions made in the test setup.\n\nThe assumption that Bob must gain a profit on both tokens was unnecessarily strict. Usually, when swapping `X quote -> Y asset -> Z quote`, the amount of Asset tokens required should be zero. This is because the second swap's input is simply defined as the first swaps output `Y asset` and normally this would mean that we are not required to provide any Asset tokens. However, in this case, due to the rounding logic in the protocol, often a minimum of `1` token is required in the process of swapping back and forth.\n\n```js\n// Bob should gain tokens.\nif (assetGain < 0 || quoteGain < 0) return;\n```\n\nFurthermore, Hyper's batched instruction processing allowed to execute more efficient swaps.\n\n```js\nfunction swapBackAndForth(bool sellAsset, uint128 input, uint128 output1, uint128 output2)\n    internal\n    returns (bool success)\n{\n    // Encode batch instructions.\n    bytes[] memory instructions = new bytes[](2);\n\n    instructions[0] = ProcessingLib.encodeSwap(0, poolId, 0, input, 0, output1, sellAsset ? 0 : 1);\n    instructions[1] = ProcessingLib.encodeSwap(0, poolId, 0, output1, 0, output2, sellAsset ? 1 : 0);\n\n    bytes memory data = ProcessingLib.encodeJumpInstruction(instructions);\n\n    (success,) = address(hyper).call(data);\n}\n```\n\nA last discovery was made when it was observed that the exploit would only be possible if the attacker was **not** the pool controller. The original idea was that the pool controller would receive a favorable fee compared to the standard swap fee. However, since the pool controller's fee was not dependent on the amount of tokens swapped, but dependent on the pool's liquidity, this created a scenario, where the exploit was not possible.\n\nThis highlighted the significance of questioning assumptions and testing varying conditions.\n\nThe Primitive Hyper Swap case offers several insights that can be applied to:\n\n1. **Establish Precise Variable Bounds**: Address potential reverts by ensuring that all variables are bounded correctly.\n2. **Augment Coverage Insight**: Introduce sanity checks to assess the feasibility of the test hypothesis, even considering the removal of protocol invariant checks.\n3. **Challenge Every Assumption**: The audit revealed how certain assumptions, like the fixed priority fee required for the exploit, or the prerequisite of token approvals, can skew testing results.\n\nIn essence, offensive testing requires a dynamic and persistent approach. It's not just about confirming the robustness of a system but actively seeking potential cracks. Only by repeatedly challenging our understanding and assumptions can we hope to uncover hidden vulnerabilities.\n\n## Conclusions and Takeaways\n\n1. **Treat your tests as production code**\n   - Tests can also have bugs.\n   - Risks: Faulty tests can lead to misleading outcomes, false confidence and mask bugs.\n2. **Understand limitations of testing and tooling**\n   - Recognize flawed or insignificant tests.\n   - Tests can only go so far in reproducing realistic scenarios.\n   - Tools can have biases or bugs, or might not be able to cover all significant input space.\n3. **Explore different testing strategies and techniques**\n   - Unit, fuzz, integration tests\n   - Test multiple properties and different angles\n   - Add redundancy\n4. **Examine assumptions, preconditions, and conclusions of tests**\n   - Check for hidden assumptions or inaccurate preconditions. (Actors in realistic scenario, inputs sorted, bytes data encoded in certain way)\n   - Do tested properties hold for all significant cases?\n5. **Test your tests**\n   - Create meta-tests.\n   - Introduce deliberate bugs to see if tests detect them.\n   - Apply mutation testing.\n\nQuestions to be asking yourself when reviewing your tests:\n\n- How can we ensure that our tests are covering essential parts of the code base?\n- How do we measure the efficacy and robustness of our tests?\n\n## Extra Material: Additional Testing Shortcomings\n\nThis section contains additional scenarios highlighting shortcomings in testing that would have made the main article too lengthy.\n\n### Testing Helper Functions\n\nThinking back to the example of the shortcomings of testing ERC721A, and how expressive fuzz tests allowing multiple ID transfers were absent. Let's examine a quick and dirty fuzz test in that vein.\n\nFor this, we'll make use of a testing helper library called `random`, which will generate a new pseudo random `uint256` number when calling `random.next()`. The library is seeded by calling `random.seed()`.\n\n```js\nlibrary random {\n    bytes32 constant RANDOM_SEED_SLOT = keccak256(\"random.seed.slot\");\n\n    /// @notice Seed randomness.\n    function seed(uint256 randomSeed) internal {\n        assembly {\n            // Store random seed.\n            sstore(RANDOM_SEED_SLOT, randomSeed)\n        }\n    }\n\n    /// @notice Generates a next random number by hashing\n    ///         the current seed value stored in `RANDOM_SEED_SLOT`.\n    function next() internal returns (uint256 nextRandom) {\n        assembly {\n            // Load random seed from storage slot.\n            let r := sload(RANDOM_SEED_SLOT)\n            // Compute keccak256 hash of seed.\n            mstore(0, r)\n            nextRandom := keccak256(0, 0x20)\n            // Update random seed.\n            sstore(RANDOM_SEED_SLOT, r)\n        }\n    }\n}\n```\n\nUsing this library we can quickly implement a function that will test `10` token transfers for all `30` minted IDs.\n\n```js\ncontract TestRandom_DONT is Test {\n    uint256 constant NUM_TRANSFERS = 10;\n    uint256 constant NUM_IDS = 30;\n\n    function test_transfer(uint256 seed) public {\n        // Seed randomness.\n        random.seed(seed);\n\n        // Create token and mint to `address(this)`.\n        ERC721 token = new ERC721A(NUM_IDS);\n\n        // Set all local token ownerships to `address(this)`.\n        address[NUM_IDS] memory owners;\n        for (uint256 i; i < NUM_IDS; i++) {\n            owners[i] = address(this);\n        }\n\n        for (uint256 i; i < NUM_TRANSFERS; i++) {\n            // Select a random id `nextId` to\n            // transfer from `currOwner` to a randomly\n            // selected `nextOwner`.\n            uint256 nextId = random.next() % NUM_IDS;\n            address nextOwner = address(uint160(random.next()));\n            address currOwner = owners[nextId];\n\n            // Transfer `nextId` from `currOwner` to `nextOwner`.\n            vm.prank(currOwner);\n            token.transferFrom(currOwner, nextOwner, nextId);\n\n            // Update local ownership data.\n            owners[nextId] = nextOwner;\n        }\n\n        // Validate final token ownerships.\n        for (uint256 i; i < NUM_IDS; i++) {\n            assertEq(owners[i], token.ownerOf(i));\n        }\n    }\n}\n```\n\nLet's start out by highlighting some hidden assumptions contained in this test that might **not** reflect real world conditions and are limitations of the test:\n\n1. All `30` tokens are minted in a single batch to a single owner.\n   1. There are no successive mints to multiple (or the same) owners.\n   2. There is no mint of variable size (e.g. `0`, `1`, ...).\n   3. There is no further mint after IDs have been transferred.\n2. The `nextId` is selected uniformly (low edge-case probability).\n   1. Selecting previous `ID` or `ID + 1` is low.\n   2. Selecting `ID = 0` or `ID = 30` is low.\n3. The `nextOwner` is selected uniformly.\n   1. The probability that the same or a previous owner is selected is negligible.\n   2. The probability that the zero address is selected is negligible.\n\nDespite all these shortcomings and biases, this fuzz test should still be able to catch the missing edge case related to the ownership bug in the ERC721A implementation from earlier. However, it won't _ever_ be able to find it. This test will pass no matter what random IDs and owners will be drawn. In fact, this test might pass for entirely degenerate ERC721A implementations as well..\n\n<details>\n<summary><b>Spot the issue above! [spoiler]</b><br><br></summary>\n\nThe issue here is that the random seed is not updated to the new value. The old previous seed is simply stored again.\n\n```diff\n            let r := sload(RANDOM_SEED_SLOT)\n            // Compute keccak256 hash of seed.\n            mstore(0, r)\n            nextRandom := keccak256(0, 0x20)\n            // Update random seed.\n-           sstore(RANDOM_SEED_SLOT, r)\n+           sstore(RANDOM_SEED_SLOT, nextRandom)\n```\n\nThis results in poor randomness. This library always returns the same random value, the same `nextTokenId` and the same `nextOwner`!\n\n_How could the mistake in the helper function have been discovered?_\n\nThe helper function is being used without any guarantees whatsoever. We require minimal tests for our helper functions to ensure that every test being built on top of it is working reliably.\n\n```js\ncontract TestRandom_DO is Test {\n    uint256 constant NUM = 30;\n\n    function test_seed(uint256 seed1, uint256 seed2) public {\n        // Require the seeds to differ.\n        vm.assume(seed1 != seed2);\n\n        // Store the first `NUM` random numbers\n        // produced by the first seed.\n        uint256[NUM] memory randomNumbers;\n\n        random.seed(seed1);\n\n        for (uint256 i; i < NUM; i++) {\n            randomNumbers[i] = random.next();\n        }\n\n        // Assert the random numbers produced\n        // by the second seed differ.\n        random.seed(seed2);\n\n        for (uint256 i; i < NUM; i++) {\n            assertTrue(randomNumbers[i] != random.next());\n        }\n\n        // Assert that resetting to the old\n        // seed produces the same random sequence.\n        random.seed(seed1);\n\n        for (uint256 i; i < NUM; i++) {\n            assertEq(randomNumbers[i], random.next());\n        }\n    }\n\n    /// @notice Marks a random number as seen.\n    mapping(uint256 => bool) randomNumberSeen;\n\n    function test_next(uint256 seed) public {\n        random.seed(seed);\n\n        for (uint256 i; i < NUM; i++) {\n            uint256 r = random.next();\n\n            // The probability of r ∈ [0, 100_000]\n            // should be negligible.\n            assertTrue(r > 100_000);\n            // The random number should be unseen.\n            assertEq(randomNumberSeen[r], false);\n            // Mark number as seen.\n            randomNumberSeen[r] = true;\n        }\n    }\n}\n```\n\nTesting the `random` library's `seed` and `next` functions in a couple fuzz tests would have detected the issue related to the bad randomness.\n\n</details>\n\nThat's why it's important to **test your helper functions** as well. Building a robust structure of tests requires a solid foundation. There is no cake without a base!\n\n### Tool Quirks\n\nLet's revisit the notion that it's important to understand how to use your tool, its biases and quirks. Below is a function `MerkleLib.zeros` that returns pre-computed roots for empty subtrees of a given depth. These have been computed beforehand and are being validated and fixed through a unit test that checks all values.\n\n```js\nuint256 constant DEPTH = 4;\n\nlibrary MerkleLib {\n    error InvalidZerosLevel();\n\n    /// @notice Returns pre-computed zero sub-tree root of depth `level`.\n    ///         Each sub-tree root is computed by:\n    ///             root(0) := keccak256(enc(0, 0))\n    ///             root(N) := keccak256(enc(root(N-1), root(N-1)))\n    function zeros(uint256 level) internal pure returns (bytes32) {\n        if (level == 0) return 0x2098f5fb9e239eab3ceac3f27b81e481dc3124d55ffed523a839ee8446b64864;\n        if (level == 1) return 0x1069673dcdb12263df301a6ff584a7ec261a44cb9dc68df067a4774460b1f1e1;\n        if (level == 2) return 0x18f43331537ee2af2e3d758d50f72106467c6eea50371dd528d57eb2b856d238;\n        if (level == 3) return 0x07f9d837cb17b0d36320ffe93ba52345f1b728571a568265caac97559dbc952a;\n        if (level == 4) return 0x2b94cf5e8746b3f5c9631f4c5df32907a699c58c94b2ad4d7b5cec1639183f55;\n        revert InvalidZerosLevel();\n    }\n}\n\ncontract TestMerkle_DONT is Test {\n    /// Test `MerkleLib.zeros` returns correct hash values.\n    function test_zeros() public {\n        bytes32 node;\n\n        for (uint256 i; i < DEPTH + 1; i++) {\n            node = keccak256(abi.encode(node, node));\n\n            assertEq(MerkleLib.zeros(i), node);\n        }\n\n        // Calling `MerkleLib.zeros(DEPTH + 1)` should revert.\n        vm.expectRevert(MerkleLib.InvalidZerosLevel.selector);\n        MerkleLib.zeros(DEPTH + 1);\n    }\n}\n```\n\nThe test `test_zeros` passes the unit test.\n\n```js\nRunning 1 test for test/TestMerkle.sol:TestMerkle_DONT\n[PASS] test_zeros() (gas: 46756)\nTest result: ok. 1 passed; 0 failed; 0 skipped; finished in 337.54µs\n```\n\n_What is wrong here?_\n\nIf you recall some of the advice previously given, then you might be able to rewrite the test in a safer way and spot the issue.\n\n<details>\n<summary><b>Spot the issue above! [spoiler]</b><br><br></summary>\n\nOnce we split up our test based on the expected outcomes (passing, reverting), we immediately see the issue at hand.\n\n```js\ncontract TestMerkle_DO is Test {\n    /// Test `MerkleLib.zeros` returns correct hash values.\n    function test_zeros() public {\n        bytes32 node;\n\n        for (uint256 i; i < DEPTH + 1; i++) {\n            node = keccak256(abi.encode(node, node));\n\n            assertEq(MerkleLib.zeros(i), node);\n        }\n    }\n\n    /// Calling `MerkleLib.zeros(DEPTH + 1)` should revert.\n    function test_zeros_revert_InvalidZerosLevel() public {\n        vm.expectRevert(MerkleLib.InvalidZerosLevel.selector);\n        MerkleLib.zeros(DEPTH + 1);\n    }\n}\n```\n\nThis produces the following output.\n\n```js\nRunning 2 tests for test/TestMerkle.sol:TestMerkle_DO\n[FAIL. Reason: Assertion failed.] test_zeros() (gas: 46138)\n[PASS] test_zeros_revert_InvalidZerosLevel() (gas: 3252)\nTest result: FAILED. 1 passed; 1 failed; 0 skipped; finished in 397.04µs\n\nRan 2 test suites: 2 tests passed, 1 failed, 0 skipped (3 total tests)\n```\n\nWe now see that the first part of the test was actually failing. All the pre-computed hashes have been calculated using the Poseidon hash instead of `keccak256` resulting in incorrect values.\n\n_Why does the test pass nonetheless?_\n\nThe reason the single test passes nonetheless when testing both things at once is due to a [quirk in Foundry](https://github.com/foundry-rs/foundry/issues/4832) related to using `vm.expectRevert` on internal functions. Failing `assert` statements are ignored if a revert is caught on an internal function.\n\n</details>\n\nRemember the advice from earlier that **a test should only be testing one thing** at a time. Again, this also makes the testing structure clearer and simpler.\n","title":"Test Your Tests!","date":"Oct 20, 2023","excerpt":"Writeup of the talks given at TrustX & Solidity Summit at Devconnect in Istanbul.","suptitle":"The Dos and Don'ts of testing","unpublished":true}},"__N_SSG":true}